{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88f46d3",
   "metadata": {},
   "source": [
    "# Installation and Import of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import whisper\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa \n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from flask import request\n",
    "from scipy.io import wavfile as wav\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7a679",
   "metadata": {},
   "source": [
    "# Prerequisite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide unwanted warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fa0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key\n",
    "with open(\"OPENAI_API_KEY.json\") as f:\n",
    "    secrets = json.load(f)\n",
    "    openai_key = secrets[\"openai\"][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with OpenAI API using API key\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Base model of Whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Check the device used by model\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c290cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary mp3 file for storing audio data if it does not exist already\n",
    "##!../anaconda3/bin/ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c4339",
   "metadata": {},
   "source": [
    "# Define function for transcribing user's voice input using OpenAI's Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for transcribing user's voice input using OpenAI's Whisper\n",
    "def transcribe(audio):\n",
    "    \n",
    "    # Load audio and trim audio to 30 sec\n",
    "    audio = whisper.load_audio(audio)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # Decode the audio\n",
    "    result = model.transcribe(audio)\n",
    "    user_text = result[\"text\"]\n",
    "    \n",
    "    return user_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb75b06",
   "metadata": {},
   "source": [
    "# Define function for generating text response using OpenAI's GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for generating text response using OpenAI's GPT\n",
    "def generate_response(prompt):\n",
    "    \n",
    "    # System defined role\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI voice assistant. Generate text responses when provided with a chat history, your text will be used as the answer using text-to-speech API\"}]\n",
    "    \n",
    "    # Generate and return the response for a user prompt\n",
    "    if prompt:\n",
    "        messages.append(\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        )\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "            )\n",
    "        \n",
    "    reply = chat.choices[0].message.content\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961db411",
   "metadata": {},
   "source": [
    "# MFCCs Extraction for Accent Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52ad50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign the filepath to a sample audio for testing\n",
    "filename = 'accentdb_extended/data/american/speaker_01/american_s01_676.wav'\n",
    "\n",
    "# Displaying the wavelenght of the filename american audio\n",
    "Librosa_data, Librosa_sample_rate = librosa.load(filename)\n",
    "librosa.display.waveshow(Librosa_data, sr=Librosa_sample_rate)\n",
    "plt.figure(figsize = (14,5))\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a705c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa converts the signal to mono, meaning the channel will always be 1\n",
    "print('Librosa sample rate = ', Librosa_sample_rate)\n",
    "print('Mono Audio of Librosa_data:', Librosa_data)\n",
    "wave_sample_rate, wave_audio = wav.read(filename)\n",
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original audio\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b79c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=Librosa_data, sr=Librosa_sample_rate, n_mfcc=20)\n",
    "print(mfccs.shape)\n",
    "type(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(mfccs, sr=Librosa_sample_rate, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for extracting the filenames of all audio files\n",
    "def feature_extractor():\n",
    "    DIR1 = 'accentdb_extended/data'\n",
    "    DIR2 = 'accentdb_core/data'\n",
    "    dir_name = os.listdir(DIR1)\n",
    "    dir2_name = os.listdir(DIR2)\n",
    "    speaker_files = []\n",
    "    #Mapping all the audiofiles at the accentdb_extend folder\n",
    "    for folder_name in dir_name:\n",
    "        for speaker in os.listdir(os.path.join(DIR1,folder_name)):\n",
    "            for audio in os.listdir(os.path.join(DIR1,folder_name,speaker)):\n",
    "                filename_speaker = os.path.join(DIR1,folder_name,speaker,audio)\n",
    "                speaker_files.append([folder_name, filename_speaker])\n",
    "    #Mapping all the audiofiles at the accentdb_core folder          \n",
    "    for folder2_name in dir2_name:\n",
    "        for speaker2 in os.listdir(os.path.join(DIR2,folder2_name)):\n",
    "            for audio2 in os.listdir(os.path.join(DIR2,folder2_name,speaker2)):\n",
    "                filename2_speaker = os.path.join(DIR2,folder2_name,speaker2,audio2)\n",
    "                speaker_files.append([folder2_name, filename2_speaker])\n",
    "        \n",
    "    return speaker_files\n",
    "\n",
    "f = feature_extractor()\n",
    "f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all MFCCS of audio data in the dataset folder\n",
    "def extract_mfcc_features(audio_path):\n",
    "    # Load audio file with Librosa\n",
    "    signal, sample_rate = librosa.load(audio_path)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=20)\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc41145",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = pd.DataFrame(f, columns = ['Speaker','audio_path'])\n",
    "print(audio_data.head())\n",
    "print(audio_data.shape)\n",
    "print(audio_data['Speaker'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00214c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Extract features for all audio files\n",
    "speaker_files = feature_extractor()\n",
    "data_extracted = []\n",
    "file_path = audio_data['audio_path'].tolist()\n",
    "audio_name = audio_data['Speaker'].tolist()\n",
    "class_number = audio_data['Speaker'].unique()\n",
    "for i in range(len(file_path)):\n",
    "    mfccs = extract_mfcc_features(file_path[i])\n",
    "    data_extracted.append([mfccs, audio_name[i]])\n",
    "    \n",
    "data = pd.DataFrame(data_extracted, columns=['Features', 'audio_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6868399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca87f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Features'].tolist()\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into independent and dependent dataset\n",
    "X = data['Features'].tolist()\n",
    "X = [np.mean(i.T,axis = 0) for i in X]\n",
    "X = np.asarray(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67bbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the y_name into a number \n",
    "y_name = [i[0] for i in f]\n",
    "y = np.array(pd.get_dummies(y_name, dtype=float))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eccaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of x_data (independent variables\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbc342",
   "metadata": {},
   "source": [
    "# ANN Modelling (4 layer Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a42f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model=Sequential()\n",
    "###first layer\n",
    "ann_model.add(Dense(100,input_shape=(20,)))\n",
    "ann_model.add(Activation('relu'))\n",
    "ann_model.add(Dropout(0.5))\n",
    "###second layer\n",
    "ann_model.add(Dense(200))\n",
    "ann_model.add(Activation('relu'))\n",
    "ann_model.add(Dropout(0.5))\n",
    "###third layer\n",
    "ann_model.add(Dense(100))\n",
    "ann_model.add(Activation('relu'))\n",
    "ann_model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "ann_model.add(Dense(num_labels))\n",
    "ann_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32509d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trianing the ANN model\n",
    "\n",
    "num_epochs = 200\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='accent_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True,patience=10)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "history = ann_model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116134fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of ANN model\n",
    "\n",
    "# Plot training and validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01780773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# Get the prediction from the X_test Dataset\n",
    "prediction_ANN = ann_model.predict(X_test)\n",
    "prediction_ANN_rounded = [np.argmax(i) for i in prediction_ANN]\n",
    "prediction_ANN_rounded[0]\n",
    "y_test_index = [np.argmax(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eadeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix - verify accuracy of each class\n",
    "cm = tf.math.confusion_matrix(labels = y_test_index, predictions = prediction_ANN_rounded)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm,annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the Prediction_ANN_rounded into a categorical\n",
    "speaker = ['american','welsh','telugu','bangla','australian','british','odiya',\n",
    " 'indian','malayalam']\n",
    "#for the prediction dataset\n",
    "predicted_audio = X_test[0]\n",
    "predicted_audio = np.expand_dims(predicted_audio, axis=0) \n",
    "# reshape for prediction\n",
    "predicted_index = np.argmax(ann_model.predict(predicted_audio), axis=-1)[0]\n",
    "predicted_speaker = speaker[predicted_index]\n",
    "\n",
    "print('Predicted Audio: ' + predicted_speaker)\n",
    "print('Test Audio: ', speaker[np.argmax(y_test[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf4300",
   "metadata": {},
   "source": [
    "# Using the trained model in voice assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6562f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all MFCCS of audio data \n",
    "def extract_mfcc_features(audio_path):\n",
    "    # Load audio file with Librosa\n",
    "    signal, sample_rate = librosa.load(audio_path)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=20)\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58929588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get to top-level domain (tld) for gTTS\n",
    "\n",
    "def get_domain(predicted_speaker):\n",
    "    if predicted_speaker == 'american':\n",
    "        return 'us', 'American'\n",
    "    elif predicted_speaker == 'australian':\n",
    "        return 'com.au', 'Australian'\n",
    "    elif predicted_speaker in ['welsh', 'british']:\n",
    "        return 'co.uk', 'British'\n",
    "    else:\n",
    "        return 'co.in', 'Indian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcadfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to predict the accent using trained model and return the tld and predicted speaker\n",
    "\n",
    "def predict_accent(file_path):\n",
    "    \n",
    "    speech_txt = transcribe(file_path)\n",
    "    # Extract mfcc features of the data\n",
    "    mfcc = extract_mfcc_features(file_path)\n",
    "    \n",
    "    # Reshape the array\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = mfcc.reshape(-1, 20)\n",
    "    # Predict the accent\n",
    "    speaker = ['american','welsh','telugu','bangla','australian','british','odiya',\n",
    "               'indian','malayalam']\n",
    "    \n",
    "    ann = load_model(\"accent_classification.hdf5\")\n",
    "    predicted_label = np.argmax(ann.predict(mfcc), axis=-1)[0]\n",
    "    predicted_speaker = speaker[predicted_label]\n",
    "\n",
    "    domain, accent = get_domain(predicted_speaker)\n",
    "\n",
    "    return (speech_txt, str(domain), accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a5b45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing the function with a sample audio file of known accent\n",
    "predict_accent('accentdb_extended/data/indian/speaker_02/indian_s02_709.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea2cc3",
   "metadata": {},
   "source": [
    "# Define function for converting text to speech using Google Cloud TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e002ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set language for text-to-speech\n",
    "language = 'en'\n",
    "\n",
    "# Define function for converting text to speech using Google Cloud TTS\n",
    "def generate_audio(out_text, accent_tld):\n",
    "    audio_obj = gTTS(text= out_text,\n",
    "                    lang = language,\n",
    "                     tld = accent_tld,\n",
    "                    slow = False)\n",
    "    audio_obj.save(\"Temp.mp3\")\n",
    "    return \"Temp.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe31638",
   "metadata": {},
   "source": [
    "# Define function for running the Voice Assistant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f178931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store session history\n",
    "history = {}\n",
    "\n",
    "# Define a function for running the Voice Assistant\n",
    "def voice_assistant(session_id, audio):\n",
    "    # Transcribe the audio and Predict the accent\n",
    "    text, accent, prediction = predict_accent(audio)\n",
    "    \n",
    "    # Check if the session ID is already in the History\n",
    "    if session_id not in history:\n",
    "        history[session_id] = []\n",
    "    \n",
    "    # Get the chat history for the session ID\n",
    "    chat_history = history[session_id]\n",
    "    \n",
    "    # Append the user's input to the chat history\n",
    "    chat_history.append(text)\n",
    "    \n",
    "    # Generate a response\n",
    "    response = generate_response(str(chat_history))\n",
    "    \n",
    "    # Append the response to the chat history\n",
    "    chat_history.append(response)\n",
    "    \n",
    "    # Generate an audio file from the response\n",
    "    audio_file = generate_audio(response, accent)\n",
    "\n",
    "    return (text, response, prediction, audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e6f94",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b34d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output1 = gr.Textbox(placeholder=\"Start recording to ask a question.\", \n",
    "                     label=\"Speech to Text\")\n",
    "output2 = gr.Textbox(placeholder=\"As an AI Voice assistant, I can remember conversations and respond to any follow-up corrections, answer questions, provide information, and generate creative content like jokes, puns, and poetry.\", \n",
    "                     label=\"GPT Reply\")\n",
    "output3 = gr.Textbox(label=\"Identified Accent:\", \n",
    "                     placeholder=\"An Accent similar to your dialect will be displayed here.\")\n",
    "output4 = gr.outputs.Audio(label=\"Audio Output\", type = \"filepath\")\n",
    "\n",
    "user_interface = gr.Interface(\n",
    "    fn=voice_assistant,\n",
    "    inputs=[gr.Textbox(label=\"Session ID\", placeholder=\"Enter a @user_name for yourself to get personalized experience. This will allow the model to respond to follow-up commands.\", type= \"text\"), gr.Audio(source=\"microphone\", type=\"filepath\", label=\"To ask another question, delete the current question and resubmit the question.\", placeholder=\"Ask a question by submitting the audio.\")],\n",
    "    outputs=[output1, output2, output3, output4],\n",
    "    #live=True,\n",
    "    title=\"Rishi's AI Voice Assistant\",\n",
    "    description=\"\"\"\n",
    "    Example questions you may ask:\n",
    "    \n",
    "    speak,\"Who is lead actor in 'Titanic'?\"\n",
    "    speak,\"Let's play a game of Hangman!\"\n",
    "    speak,\"What is 13 times 27.6?\"\n",
    "    speak,\"Can you help me make a comic?\"\n",
    "    \"\"\",\n",
    "    allow_flagging=\"never\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791c747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(x)=(pret, pvol, pret/pvol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04007adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "staistics(x)[0]= [(pret, pvol, pret/pvol)[1],(pret, pvol, pret/pvol)[2],(pret, pvol, pret/pvol)...,(pret, pvol, pret/pvol)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81564012",
   "metadata": {},
   "outputs": [],
   "source": [
    "run sco minimise\n",
    "\n",
    "results= res[x]\n",
    "res[x] = [(w,r), (w,r), (w,r),...,(w,r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b76b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff7c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d12ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(weights, riskFree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
